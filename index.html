<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>WGSL Shader Game</title>
    <style>
      body {
        margin: 0;
        display: flex;
        justify-content: center;
        align-items: center;
        height: 100vh;
        background: #000;
      }
      a {
        color: inherit;
      }
      canvas {
        max-width: 100%;
        max-height: 100vh;
      }
      #info {
        position: absolute;
        top: 10px;
        left: 10px;
        background: rgba(0, 0, 0, 0.7);
        padding: 10px;
        color: white;
        font-family: monospace;
      }
    </style>
  </head>
  <body>
    <div id="info">
      Arrow keys or WASD to move<br />Hit the edges for sound!<br />You can view
      the <a href="game.wgsl">source</a>.
    </div>
    <canvas id="canvas" width="800" height="600"></canvas>

    <script type="module">
      // Parse all metadata from shader
      function parseMetadata(code) {
        const titleRegex = /\/\*\*\s*@title\s+(.+?)\s*\*\//;
        const textureRegex = /\/\*\*\s*@asset\s+texture\s+([^\s]+)\s*\*\//g;
        const soundRegex = /\/\*\*\s*@asset\s+sound\s+([^\s]+)\s*\*\//g;

        const titleMatch = titleRegex.exec(code);
        const title = titleMatch ? titleMatch[1] : "WGSL Shader Game";

        const textures = [];
        const sounds = [];

        let match;
        while ((match = textureRegex.exec(code)) !== null) {
          textures.push(match[1]);
        }
        while ((match = soundRegex.exec(code)) !== null) {
          sounds.push(match[1]);
        }

        return { title, textures, sounds };
      }

      // Simple shader preprocessor with /** @include ... */ support
      async function preprocessShader(
        code,
        basePath = "",
        visited = new Set(),
      ) {
        const includeRegex = /\/\*\*\s*@include\s+([^\s]+)\s*\*\//g;
        const includes = [];
        let match;

        while ((match = includeRegex.exec(code)) !== null) {
          includes.push({
            full: match[0],
            path: match[1],
            index: match.index,
          });
        }

        // Process includes in reverse order to maintain string indices
        for (let i = includes.length - 1; i >= 0; i--) {
          const inc = includes[i];
          const fullPath = basePath ? `${basePath}/${inc.path}` : inc.path;

          // Prevent circular includes
          if (visited.has(fullPath)) {
            throw new Error(`Circular include detected: ${fullPath}`);
          }

          visited.add(fullPath);

          // Load and preprocess the included file
          const includeCode = await fetch(fullPath).then((r) => r.text());
          const processedInclude = await preprocessShader(
            includeCode,
            fullPath.substring(0, fullPath.lastIndexOf("/")),
            new Set(visited),
          );

          // Replace include statement with the file contents
          code =
            code.substring(0, inc.index) +
            `// --- Begin include: ${inc.path} ---\n` +
            processedInclude +
            `\n// --- End include: ${inc.path} ---\n` +
            code.substring(inc.index + inc.full.length);
        }

        return code;
      }

      // Parse assets from shader
      function parseAssets(code) {
        const textureRegex = /\/\*\*\s*@asset\s+texture\s+([^\s]+)\s*\*\//g;
        const soundRegex = /\/\*\*\s*@asset\s+sound\s+([^\s]+)\s*\*\//g;

        const textures = [];
        const sounds = [];

        let match;
        while ((match = textureRegex.exec(code)) !== null) {
          textures.push(match[1]);
        }
        while ((match = soundRegex.exec(code)) !== null) {
          sounds.push(match[1]);
        }

        return { textures, sounds };
      }

      // Load audio file
      async function loadSound(audioContext, url) {
        const response = await fetch(url);
        const arrayBuffer = await response.arrayBuffer();
        return await audioContext.decodeAudioData(arrayBuffer);
      }

      // Play sound
      function playSound(audioContext, buffer) {
        const source = audioContext.createBufferSource();
        source.buffer = buffer;
        source.connect(audioContext.destination);
        source.start();
      }

      const canvas = document.getElementById("canvas");
      const adapter = await navigator.gpu?.requestAdapter();
      if (!adapter) {
        alert("WebGPU not supported");
        throw new Error("No WebGPU");
      }

      const device = await adapter.requestDevice();
      const context = canvas.getContext("webgpu");
      const format = navigator.gpu.getPreferredCanvasFormat();
      context.configure({ device, format });

      // Input state
      const buttons = new Uint32Array(12);
      const keyMap = {
        ArrowUp: 0,
        w: 0,
        W: 0,
        ArrowDown: 1,
        s: 1,
        S: 1,
        ArrowLeft: 2,
        a: 2,
        A: 2,
        ArrowRight: 3,
        d: 3,
        D: 3,
      };

      addEventListener("keydown", (e) => {
        if (keyMap[e.key] !== undefined) {
          buttons[keyMap[e.key]] = 1;
          e.preventDefault();
        }
      });
      addEventListener("keyup", (e) => {
        if (keyMap[e.key] !== undefined) {
          buttons[keyMap[e.key]] = 0;
          e.preventDefault();
        }
      });

      // Load shader and parse metadata
      let code = await fetch("game.wgsl").then((r) => r.text());
      code = await preprocessShader(code);
      const metadata = parseMetadata(code);

      // Set window title
      document.title = metadata.title;

      console.log("Loading:", metadata);

      const module = device.createShaderModule({ code });
      const assets = parseAssets(code);

      console.log("Loading assets:", assets);

      // Setup Web Audio
      const audioContext = new AudioContext();
      const sounds = {};
      for (const soundFile of assets.sounds) {
        sounds[soundFile] = await loadSound(audioContext, soundFile);
      }

      // Load texture
      const img = await createImageBitmap(
        await fetch(assets.textures[0]).then((r) => r.blob()),
      );
      const texture = device.createTexture({
        size: [img.width, img.height],
        format: "rgba8unorm",
        usage:
          GPUTextureUsage.TEXTURE_BINDING |
          GPUTextureUsage.COPY_DST |
          GPUTextureUsage.RENDER_ATTACHMENT,
      });
      device.queue.copyExternalImageToTexture({ source: img }, { texture }, [
        img.width,
        img.height,
      ]);

      const sampler = device.createSampler({
        magFilter: "linear",
        minFilter: "linear",
      });

      // Buffers
      const inputBuffer = device.createBuffer({
        size: 64,
        usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
      });

      const stateBuffer = device.createBuffer({
        size: 24,
        usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
      });
      device.queue.writeBuffer(
        stateBuffer,
        0,
        new Float32Array([400, 300, 0, 0, 0, 0]),
      );

      // Create audio triggers buffer
      const audioBuffer = device.createBuffer({
        size: 4,
        usage:
          GPUBufferUsage.STORAGE |
          GPUBufferUsage.COPY_SRC |
          GPUBufferUsage.COPY_DST,
      });
      device.queue.writeBuffer(audioBuffer, 0, new Uint32Array([0]));

      // Create staging buffer to read audio triggers
      const audioReadBuffer = device.createBuffer({
        size: 4,
        usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST,
      });

      // Pipelines with auto layout
      const computePipeline = device.createComputePipeline({
        layout: "auto",
        compute: { module, entryPoint: "update" },
      });

      const renderPipeline = device.createRenderPipeline({
        layout: "auto",
        vertex: { module, entryPoint: "vs_main" },
        fragment: { module, entryPoint: "fs_render", targets: [{ format }] },
        primitive: { topology: "triangle-list" },
      });

      // Bind groups - compute uses group 0
      const computeBindGroup = device.createBindGroup({
        layout: computePipeline.getBindGroupLayout(0),
        entries: [
          { binding: 0, resource: { buffer: inputBuffer } },
          { binding: 1, resource: { buffer: stateBuffer } },
          { binding: 2, resource: { buffer: audioBuffer } },
        ],
      });

      // Render uses group 0 (textures) and group 1 (state)
      const renderTextureBindGroup = device.createBindGroup({
        layout: renderPipeline.getBindGroupLayout(0),
        entries: [
          { binding: 0, resource: texture.createView() },
          { binding: 1, resource: sampler },
        ],
      });

      const renderStateBindGroup = device.createBindGroup({
        layout: renderPipeline.getBindGroupLayout(1),
        entries: [{ binding: 0, resource: { buffer: stateBuffer } }],
      });

      // Render loop
      let last = performance.now();
      let firstFrame = true;
      let lastBumpTrigger = 0;
      let audioReadPending = false;

      function frame() {
        const now = performance.now();
        let dt = (now - last) / 1000;
        last = now;

        // Cap delta time to prevent huge jumps during lag
        if (firstFrame) {
          dt = 1 / 60; // Assume 60fps for first frame
          firstFrame = false;
        } else {
          dt = Math.min(dt, 0.1); // Cap at 100ms (10fps minimum)
        }

        // Create input data with proper alignment
        const inputData = new ArrayBuffer(64);
        const inputU32 = new Uint32Array(inputData);
        const inputF32 = new Float32Array(inputData);

        // Write buttons as vec4<u32> (indices 0-11 mapped to 0-2 vec4s)
        for (let i = 0; i < 12; i++) {
          inputU32[i] = buttons[i];
        }

        // Write time/delta/screen as f32 (indices 12-15)
        inputF32[12] = now / 1000;
        inputF32[13] = dt;
        inputF32[14] = canvas.width;
        inputF32[15] = canvas.height;

        device.queue.writeBuffer(inputBuffer, 0, inputData);

        const encoder = device.createCommandEncoder();

        const compute = encoder.beginComputePass();
        compute.setPipeline(computePipeline);
        compute.setBindGroup(0, computeBindGroup);
        compute.dispatchWorkgroups(1);
        compute.end();

        const render = encoder.beginRenderPass({
          colorAttachments: [
            {
              view: context.getCurrentTexture().createView(),
              loadOp: "clear",
              storeOp: "store",
            },
          ],
        });
        render.setPipeline(renderPipeline);
        render.setBindGroup(0, renderTextureBindGroup);
        render.setBindGroup(1, renderStateBindGroup);
        render.draw(3);
        render.end();

        device.queue.submit([encoder.finish()]);

        // Check audio triggers (non-blocking)
        if (!audioReadPending) {
          audioReadPending = true;

          const readEncoder = device.createCommandEncoder();
          readEncoder.copyBufferToBuffer(audioBuffer, 0, audioReadBuffer, 0, 4);
          device.queue.submit([readEncoder.finish()]);

          audioReadBuffer.mapAsync(GPUMapMode.READ).then(() => {
            const data = new Uint32Array(audioReadBuffer.getMappedRange());
            const bumpTrigger = data[0];

            if (bumpTrigger > lastBumpTrigger) {
              playSound(audioContext, sounds["bump.ogg"]);
              lastBumpTrigger = bumpTrigger;
            }

            audioReadBuffer.unmap();
            audioReadPending = false;
          });
        }

        requestAnimationFrame(frame);
      }
      frame();
    </script>
  </body>
</html>
